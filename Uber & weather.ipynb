{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "import plotly.offline as po\n",
    "import plotly.graph_objs as go\n",
    "import ipywidgets as widegets\n",
    "from scipy import special\n",
    "\n",
    "py.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime \n",
    "import time \n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from pylab import *\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "plt.rcParams[\"figure.figsize\"] = [9,5]\n",
    "sns.set(rc={'figure.figsize':(6,3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_df=pd.read_csv('cab_rides.csv')\n",
    "weather_df=pd.read_csv('weather.csv')\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting UNIX Date to DatetimeNS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_df['date_time_ns'] = pd.to_datetime(cab_df['time_stamp']/1000, unit='s')\n",
    "weather_df['date_time_ns'] = pd.to_datetime(weather_df['time_stamp'], unit='s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['date_time_ns'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['date_time_ns'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_df['date_time_ns'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zscore(series):\n",
    "    return (series - series.mean())/series.std()\n",
    "\n",
    "def mean(series):\n",
    "    return series.sum()/series.count()\n",
    "\n",
    "def log_function(series):\n",
    "    return np.log(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing datasets in order to merge them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Primary Key taking (Location, Date & Time) from respective datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_df['merge_col'] = cab_df.source.astype(str) +\" - \"+ cab_df.date_time_ns.dt.date.astype(\"str\") +\" - \"+ cab_df.date_time_ns.dt.hour.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['merge_col'] = weather_df.location.astype(str) +\" - \"+ weather_df.date_time_ns.dt.date.astype(\"str\") +\" - \"+ weather_df.date_time_ns.dt.hour.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.index = weather_df['merge_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['merge_col'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateRowsDF = weather_df.drop_duplicates(keep='last', subset='merge_col', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate Rows except first occurrence based on all columns are :\")\n",
    "print(duplicateRowsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = cab_df.join(weather_df,on=['merge_col'],rsuffix ='_w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling 0 for missing values for 'Rain' (Missing Data at Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['rain'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observing from the Null value plot, looks like we received some redundancies while merging the datasets on the primary key. We can dop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[pd.notnull(merged_df['date_time_ns_w'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "sns.heatmap(merged_df.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df['cab_type'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching | 'Hour' | 'Day of Week' | 'Month' | 'Day in Month' | from Time Stamp of the cab bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['hour'] = merged_df.date_time_ns.dt.hour\n",
    "merged_df['dayofweek'] = merged_df.date_time_ns.dt.dayofweek\n",
    "merged_df['dayofmonth'] = merged_df.date_time_ns.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.set_index('date_time_ns_w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10,3]\n",
    "\n",
    "merged_df.groupby(['hour', 'cab_type'])['id'].count().unstack().plot(kind='line', marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df_peak=merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10,3]\n",
    "\n",
    "merged_df.groupby(['hour', 'cab_type'])['price'].mean().unstack().plot(kind='line', marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(merged_df, x='hour', y='price', color='cab_type',histfunc='avg', barmode='group')\n",
    "po.plot(fig, filename = 'price vs hour.html_avg', auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(merged_df, x='hour', y='price', color='cab_type',histfunc='max', barmode='group')\n",
    "\n",
    "po.plot(fig, filename = 'price vs hour_max.html', auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='hour',y='price', data=merged_df, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['highsurge']=merged_df['surge_multiplier'].apply(lambda x:'Yes' if x>1 else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(merged_df.sort_values(by=['hour']), x='hour', y='price', \n",
    "               color='highsurge', marginal_y='box', marginal_x='histogram')\n",
    "po.plot(fig, filename = 'price vs hour.html', auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(merged_df.sort_values(by=['dayofweek']), x='dayofweek', y='price', \n",
    "               color='highsurge', marginal_y='box', marginal_x='histogram', trendline='ols')\n",
    "po.plot(fig, filename = 'price vs day.html', auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter_3d(merged_df.sort_values(by=['hour']), x='hour', y='price', z='dayofweek'\n",
    "               , color='highsurge')\n",
    "po.plot(fig, filename = 'price vs hour vs day.html', auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [14,8]\n",
    "cab_corr = merged_df.corr()\n",
    "sns.heatmap(cab_corr, cmap='YlGnBu', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(merged_df, \n",
    "            vars=['price', 'distance'],\n",
    "            hue='cab_type', \n",
    "            palette='husl',\n",
    "            plot_kws={'alpha':0.8},\n",
    "            size=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10,3]\n",
    "df_check = merged_df.loc['2018-11-26':'2018-12-18', :]\n",
    "merged_df.groupby(['hour', 'cab_type'])['surge_multiplier'].max().unstack().plot(kind='line', marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['cab_type']=='Uber'].groupby('hour')['surge_multiplier'].agg(np.max).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_lyft = merged_df[merged_df.loc[:,'cab_type']=='Lyft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_null=merged_df[merged_df['price'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_notnull=merged_df[merged_df['price'].notnull()]\n",
    "merged_df_notnull.set_index('date_time_ns_w', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_notnull.drop(columns=['time_stamp_w','time_stamp','date_time_ns'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "sns.heatmap(merged_df_notnull.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "sns.set(rc={'figure.figsize':(16,9)})\n",
    "sns.swarmplot(x='cab_type', y='price', data=merged_df_notnull, hue='hour',size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_notnull['cab_type'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dict = {}\n",
    "for col in merged_df_notnull.dtypes[merged_df_notnull.dtypes==\"object\"].index:\n",
    "    print(col)\n",
    "    le_dict[col] = preprocessing.LabelEncoder()\n",
    "    merged_df_notnull[col] = le_dict[col].fit_transform(merged_df_notnull[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(merged_df_notnull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features=scaler.transform(merged_df_notnull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = pd.DataFrame(scaled_features)\n",
    "scaled_df.columns = merged_df_notnull.columns\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lasso_regressor(X,y, names):\n",
    "    \n",
    "    lasso = Lasso(alpha=0.1)\n",
    "\n",
    "    lasso_coef = lasso.fit(X_new, y_new).coef_\n",
    "\n",
    "    plt.plot(range(len(names)), lasso_coef)\n",
    "\n",
    "    plt.xticks(range(len(names)), names, rotation=90)\n",
    "\n",
    "    plt.ylabel('Coefficients')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_forest_regressor(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                        test_size=0.3, \n",
    "                                                        random_state=21)\n",
    "\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=500,\n",
    "                               min_samples_leaf=0.1,\n",
    "                               random_state=21)\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = merged_df_notnull.drop('price', axis=1).columns\n",
    "X_new = merged_df_notnull.drop('price', axis=1).values\n",
    "y_new = merged_df_notnull['price'].values\n",
    "lasso_regressor(X_new, y_new, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df_notnull.drop('price', axis=1).values\n",
    "y = merged_df_notnull['price'].values\n",
    "\n",
    "rf = random_forest_regressor(X,y)\n",
    "importances_rf = pd.Series(rf.feature_importances_, index=merged_df_notnull.drop('price', axis=1).columns)\n",
    "\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df_notnull[['distance','surge_multiplier','hour']].values\n",
    "y = merged_df_notnull[['price']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_cv(grid_dt, X_train, X_test, y_train, y_test):\n",
    "    grid_dt.fit(X_train, y_train)\n",
    "    best_hyperparams = grid_dt.best_params_\n",
    "    best_cv_score = grid_dt.best_score_\n",
    "    best_model = grid_dt.best_estimator_ \n",
    "    test_acc = best_model.score(X_test, y_test)\n",
    "    print(best_hyperparams)\n",
    "    print(best_cv_score)\n",
    "    print(best_model)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingRegressor(random_state=21)\n",
    "params_dt = {\n",
    "    'n_estimators': [100,300,500,700],\n",
    "    'min_samples_leaf': [3, 4, 6, 8],\n",
    "    'max_depth':[3,4,5,6],\n",
    "    'learning_rate': [0.10,0.5,0, 0.0012]\n",
    "}\n",
    "\n",
    "\n",
    "grid_dt = GridSearchCV(estimator=gbt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='r2',\n",
    "                       cv=10,\n",
    "                       n_jobs=-1\n",
    "                      )\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_grid_search_cv(grid_dt, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=21)\n",
    "\n",
    "params_dt = {\n",
    "    'n_estimators': [100,300,500,700],\n",
    "    'min_samples_leaf': [0.04, 0.06, 0.08]\n",
    "}\n",
    "\n",
    "\n",
    "grid_dt = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='r2',\n",
    "                       cv=10,\n",
    "                       n_jobs=1\n",
    "                      )\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1)\n",
    "\n",
    "perform_grid_search_cv(grid_dt, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "params_dt = {\n",
    "    'max_depth':[3,4,5,6],\n",
    "    'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "    'max_features':[0.2,0.4,0.6,0.8]\n",
    "}\n",
    "\n",
    "\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='r2',\n",
    "                       cv=10,\n",
    "                       n_jobs=1\n",
    "                      )\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1)\n",
    "\n",
    "perform_grid_search_cv(grid_dt, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df_notnull[['distance','surge_multiplier','hour']].values\n",
    "y = merged_df_notnull[['price']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
